import json
from collections import defaultdict
from typing import List

from dotenv import load_dotenv
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, AIMessage
from langgraph.prebuilt import ToolInvocation, ToolExecutor

from chains import parser
from schemas import AnswerQuestion, Reflection

load_dotenv()

# Defining the Tavily search API as a tool
search = TavilySearchAPIWrapper()
tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)

# Allows parallel tool calling
tool_executor = ToolExecutor([tavily_tool])

# LangGraph node to execute tools
def execute_tools(state: List[BaseMessage]) -> List[ToolMessage]:
    # Get the tool_invocation message generated by the LLM and
    # convert it into JSON
    tool_invocation: AIMessage = state[-1]
    parsed_tool_calls = parser.invoke(tool_invocation)

    ids = []
    tool_invocations = []

    # For each tool call invocation the LLM provided, create and collect a ToolInvocation
    # object.
    for parsed_call in parsed_tool_calls:
        for query in parsed_call["args"]["search_queries"]:
            tool_invocations.append(ToolInvocation(
                tool="tavily_search_results_json",
                tool_input=query
            ))
            ids.append(parsed_call["id"])

    # Execute the requested invocations
    outputs = tool_executor.batch(tool_invocations)

    # Mapping each tool input to its corresponding output, under the call id.
    # Each id represent a tool call (in our case there will be a single tool - Tavily).
    outputs_map = defaultdict(dict)
    for id_, output, invocation in zip(ids, outputs, tool_invocations):
        outputs_map[id_][invocation.tool_input] = output

    # Wrap tool outputs in a ToolMessage. Using a loop in case there are more than
    # a single tool called.
    tool_messages = []
    for id_, mapped_output in outputs_map.items():
        tool_messages.append(
            ToolMessage(content=json.dumps(mapped_output), tool_call_id=id_)
        )

    # Since this is a LangGraph node, these will be added to the graph's state
    return tool_messages

if __name__ == "__main__":
    print ("Tool executor")

    human_message = HumanMessage(
        content="Write about AI-Powered SOC / autonomous soc problem domain,"
        " list startups that do that and raised capital."
    )

    answer = AnswerQuestion(
        answer="",
        reflection=Reflection(missing="", superfluous=""),
        search_queries=[
            'AI SOC startups funding rounds',
            'Challenges faced by AI-powered SOCs',
            'Technological innovations in autonomous SOCs'
        ],
        id="call_lkdjalksjdlkjsadJkslak"
    )

    raw_res = execute_tools(
        state=[
            human_message,
            AIMessage(
                content="",
                tool_calls=[
                    {
                        "name": AnswerQuestion.__name__,
                        "args": answer.dict(),
                        "id": "call_lkdjalksjdlkjsadJkslak"
                    }
                ]
            )
        ]
    )

    print (raw_res)